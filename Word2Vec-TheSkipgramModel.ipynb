{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec - The Skipgram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the sentence :\n",
    "\"Hello my name is Gitesh.\"\n",
    "\n",
    "With a window size of 1, we have the dataset:\n",
    "([With,window],a), ([a,size],window)....\n",
    "\n",
    "Skipgram model tries to predict each context word from its target word, and so the task becomes to predict 'with' and 'window' from 'a' and to predict 'a' and 'size' from 'window' and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "matplotlib.use('TKAgg')\n",
    "from matplotlib import pyplot as plt\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants\n",
    "batch_size = 128\n",
    "vocabulary_size = 50000\n",
    "embedding_size = 128 #Dimension of the embedding vector\n",
    "num_sampled = 64 #Number of negative examples to sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train data!\n",
      "Loaded val data!\n",
      "Loaded reverse dictionary!\n",
      "Number of training examples:  3840000\n",
      "Number of validation examples:  16\n",
      "(array([ 3081,  3081,    12,    12,     6,     6,   195,   195,     2,\n",
      "           2,  3134,  3134,    46,    46,    59,    59,   156,   156,\n",
      "         128,   128,   742,   742,   477,   477, 10572, 10572,   134,\n",
      "         134,     1,     1, 27350, 27350,     2,     2,     1,     1,\n",
      "         103,   103,   855,   855,     3,     3,     1,     1, 15068,\n",
      "       15068,     0,     0,     2,     2,     1,     1,   151,   151,\n",
      "         855,   855,  3581,  3581,     1,     1,   195,   195,    11,\n",
      "          11,   191,   191,    59,    59,     5,     5,     6,     6,\n",
      "       10713, 10713,   215,   215,     7,     7,  1325,  1325,   105,\n",
      "         105,   455,   455,    20,    20,    59,    59,  2732,  2732,\n",
      "         363,   363,     7,     7,  3673,  3673,     1,     1,   709,\n",
      "         709,     2,     2,   372,   372,    27,    27,    41,    41,\n",
      "          37,    37,    54,    54,   540,   540,    98,    98,    12,\n",
      "          12,     6,     6,  1424,  1424,  2758,  2758,    19,    19,\n",
      "         568,   568]), array([[ 5234],\n",
      "       [   12],\n",
      "       [ 3081],\n",
      "       [    6],\n",
      "       [  195],\n",
      "       [   12],\n",
      "       [    6],\n",
      "       [    2],\n",
      "       [  195],\n",
      "       [ 3134],\n",
      "       [   46],\n",
      "       [    2],\n",
      "       [ 3134],\n",
      "       [   59],\n",
      "       [  156],\n",
      "       [   46],\n",
      "       [  128],\n",
      "       [   59],\n",
      "       [  156],\n",
      "       [  742],\n",
      "       [  128],\n",
      "       [  477],\n",
      "       [10572],\n",
      "       [  742],\n",
      "       [  134],\n",
      "       [  477],\n",
      "       [10572],\n",
      "       [    1],\n",
      "       [  134],\n",
      "       [27350],\n",
      "       [    2],\n",
      "       [    1],\n",
      "       [    1],\n",
      "       [27350],\n",
      "       [  103],\n",
      "       [    2],\n",
      "       [    1],\n",
      "       [  855],\n",
      "       [    3],\n",
      "       [  103],\n",
      "       [    1],\n",
      "       [  855],\n",
      "       [15068],\n",
      "       [    3],\n",
      "       [    1],\n",
      "       [    0],\n",
      "       [    2],\n",
      "       [15068],\n",
      "       [    0],\n",
      "       [    1],\n",
      "       [    2],\n",
      "       [  151],\n",
      "       [    1],\n",
      "       [  855],\n",
      "       [  151],\n",
      "       [ 3581],\n",
      "       [  855],\n",
      "       [    1],\n",
      "       [ 3581],\n",
      "       [  195],\n",
      "       [   11],\n",
      "       [    1],\n",
      "       [  195],\n",
      "       [  191],\n",
      "       [   11],\n",
      "       [   59],\n",
      "       [  191],\n",
      "       [    5],\n",
      "       [   59],\n",
      "       [    6],\n",
      "       [10713],\n",
      "       [    5],\n",
      "       [  215],\n",
      "       [    6],\n",
      "       [10713],\n",
      "       [    7],\n",
      "       [ 1325],\n",
      "       [  215],\n",
      "       [  105],\n",
      "       [    7],\n",
      "       [ 1325],\n",
      "       [  455],\n",
      "       [   20],\n",
      "       [  105],\n",
      "       [  455],\n",
      "       [   59],\n",
      "       [ 2732],\n",
      "       [   20],\n",
      "       [   59],\n",
      "       [  363],\n",
      "       [    7],\n",
      "       [ 2732],\n",
      "       [ 3673],\n",
      "       [  363],\n",
      "       [    7],\n",
      "       [    1],\n",
      "       [ 3673],\n",
      "       [  709],\n",
      "       [    1],\n",
      "       [    2],\n",
      "       [  709],\n",
      "       [  372],\n",
      "       [   27],\n",
      "       [    2],\n",
      "       [   41],\n",
      "       [  372],\n",
      "       [   27],\n",
      "       [   37],\n",
      "       [   54],\n",
      "       [   41],\n",
      "       [   37],\n",
      "       [  540],\n",
      "       [   54],\n",
      "       [   98],\n",
      "       [   12],\n",
      "       [  540],\n",
      "       [   98],\n",
      "       [    6],\n",
      "       [ 1424],\n",
      "       [   12],\n",
      "       [ 2758],\n",
      "       [    6],\n",
      "       [ 1424],\n",
      "       [   19],\n",
      "       [ 2758],\n",
      "       [  568],\n",
      "       [   19],\n",
      "       [  687]]))\n"
     ]
    }
   ],
   "source": [
    "#Loading the dataset\n",
    "\n",
    "train_data, val_data, reverse_dictionary = load_data()\n",
    "print('Number of training examples: ', len(train_data)* batch_size)\n",
    "print('Number of validation examples: ', len(val_data))\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the computational graph/Model\n",
    "\n",
    "def skipgram():\n",
    "    batch_inputs = tf.placeholder(tf.int32, shape = [batch_size,])\n",
    "    batch_labels = tf.placeholder(tf.int32, shape = [batch_size,1])\n",
    "    val_dataset = tf.constant(val_data, dtype=tf.int32)\n",
    "    \n",
    "    with tf.variable_scope('word2vec') as scope:\n",
    "        embeddings = tf.Variable(tf.random_uniform([vocabulary_size,embedding_size],-1.0,1.0))\n",
    "        batch_embeddings = tf.nn.embedding_lookup(embeddings, batch_inputs)\n",
    "        \n",
    "        weights = tf.Variable(tf.truncated_normal([vocabulary_size,embedding_size],stddev=1.0/math.sqrt(embedding_size)))\n",
    "        biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "        loss = tf.reduce_mean(tf.nn.nce_loss(weights=weights,\n",
    "                                             biases = biases,\n",
    "                                             labels = batch_labels,\n",
    "                                             inputs = batch_embeddings,\n",
    "                                             num_sampled = num_sampled,\n",
    "                                             num_classes = vocabulary_size))\n",
    "        \n",
    "        norm= tf.sqrt(tf.reduce_mean(tf.square(embeddings), axis = 1, keep_dims = True))\n",
    "        normalized_embeddings = embeddings/norm\n",
    "        \n",
    "        val_embeddings = tf.nn.embedding_lookup(normalized_embeddings, val_dataset)\n",
    "        similarity = tf.matmul(val_embeddings, normalized_embeddings, transpose_b=True)\n",
    "        \n",
    "        return batch_inputs, batch_labels, normalized_embeddings, loss, similarity\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    batch_inputs, batch_labels, normalized_embeddings, loss, similarity = skipgram()\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        average_loss = 0.0\n",
    "        for step, batch_data in enumerate(train_data):\n",
    "            inputs, labels = batch_data\n",
    "            feed_dict = {batch_inputs: inputs, batch_labels: labels}\n",
    "            _,loss_val = sess.run([optimizer, loss], feed_dict)\n",
    "            average_loss = average_loss + loss_val\n",
    "            \n",
    "            if step%1000 == 0 :\n",
    "                if step > 0:\n",
    "                    average_loss /= 1000\n",
    "                print('Loss at iter',step,':',average_loss)\n",
    "                average_loss = 0\n",
    "                \n",
    "            if step%5000 == 0:\n",
    "                sim = similarity.eval()\n",
    "                for i in range(len(val_data)):\n",
    "                    top_k = 8\n",
    "                    nearest = (-sim[i,:]).argsort()[1:top_k+1]\n",
    "                    print_closest_words(val_data[i], nearest, reverse_dictionary)\n",
    "                    \n",
    "        final_embeddings = normalized_embeddings.eval()\n",
    "        \n",
    "        return final_embeddings\n",
    "                    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iter 0 : 311.9130859375\n",
      "Nearest to he: luminance, expending, ballast, pundit, signatory, uniting, doyle, pva,\n",
      "Nearest to called: threats, membrane, ifc, responds, roms, yakuza, bentine, rohypnol,\n",
      "Nearest to for: sir, placed, matth, modular, cypher, having, fagan, dunraven,\n",
      "Nearest to if: mauryan, misrepresented, flaps, rims, netherlands, erupted, sparrow, jehoahaz,\n",
      "Nearest to state: candidate, internalized, annoyed, doctorow, shem, nationally, facilities, cervix,\n",
      "Nearest to that: fermium, antares, unexplained, distributes, ambassadors, disturb, spartan, mondo,\n",
      "Nearest to no: luria, dreamcast, softer, indicating, spook, broch, offshoot, apostasy,\n",
      "Nearest to d: pn, tunny, trolling, teen, cloning, camelot, tractate, brake,\n",
      "Nearest to however: hoc, lex, thaddeus, locrian, foreshadowed, rodin, japheth, chaffee,\n",
      "Nearest to system: mpr, franchised, himmler, commissioned, taiga, stein, localization, amenable,\n",
      "Nearest to as: sutras, coffey, ym, box, reshaped, uneventful, lodovico, sarah,\n",
      "Nearest to there: servlet, kirby, enormously, robot, darkly, khwarizmi, requirement, article,\n",
      "Nearest to its: service, cygni, germans, flatly, tumours, artcyclopedia, asroc, markschies,\n",
      "Nearest to only: tripled, disagreeable, hopwood, landry, dew, regarding, courtesan, mindset,\n",
      "Nearest to during: billion, garden, facilities, nielsen, validly, interstellar, nosferatu, presented,\n",
      "Nearest to with: waiting, cirque, caught, proposed, reintegrated, pascal, halliwell, midrashic,\n",
      "Loss at iter 1000 : 145.24012925338747\n",
      "Loss at iter 2000 : 83.44488038635254\n",
      "Loss at iter 3000 : 59.546760180950166\n",
      "Loss at iter 4000 : 45.03699917936325\n",
      "Loss at iter 5000 : 35.94747106695175\n",
      "Nearest to he: gland, she, victoriae, molecules, uniting, design, spirits, pioneered,\n",
      "Nearest to called: threats, victoriae, membrane, taken, parentheses, hcl, six, basins,\n",
      "Nearest to for: of, having, in, placed, austin, sir, arts, and,\n",
      "Nearest to if: netherlands, clinton, biscuit, studios, erupted, winter, aristotle, modern,\n",
      "Nearest to state: candidate, facilities, vs, sea, mujahidin, continental, species, modal,\n",
      "Nearest to that: cl, amphibians, mulcahy, linebackers, rafter, crossing, treaty, be,\n",
      "Nearest to no: vs, approaches, indicating, max, austin, served, and, proposal,\n",
      "Nearest to d: failure, pbs, objectivist, soldier, soroban, moment, asa, future,\n",
      "Nearest to however: american, information, vs, people, would, gold, imitation, weak,\n",
      "Nearest to system: motile, wide, austin, performed, reginae, jewish, crash, finding,\n",
      "Nearest to as: box, in, makes, historical, matches, and, cl, by,\n",
      "Nearest to there: effect, reginae, article, activities, governors, mya, players, next,\n",
      "Nearest to its: service, germans, vs, women, cl, zone, codes, austin,\n",
      "Nearest to only: zero, areas, victoriae, vs, regarding, shortly, industry, russell,\n",
      "Nearest to during: vs, garden, billion, goal, facilities, ada, extends, presented,\n",
      "Nearest to with: proposed, in, ch, and, caught, quite, jung, extraction,\n",
      "Loss at iter 6000 : 30.38386537694931\n",
      "Loss at iter 7000 : 24.756507617473602\n",
      "Loss at iter 8000 : 22.390042909145354\n",
      "Loss at iter 9000 : 19.32206118297577\n",
      "Loss at iter 10000 : 16.327328642845153\n",
      "Nearest to he: she, it, ballast, molecules, committee, reaches, pottery, gland,\n",
      "Nearest to called: membrane, threats, victoriae, mineral, six, axis, taken, boasted,\n",
      "Nearest to for: in, and, of, having, qutb, placed, austin, sir,\n",
      "Nearest to if: netherlands, clinton, biscuit, row, winter, erupted, should, station,\n",
      "Nearest to state: candidate, facilities, mujahidin, vs, pottery, continental, built, euthanasia,\n",
      "Nearest to that: gate, in, and, rafter, sheridan, polynesian, treaty, altenberg,\n",
      "Nearest to no: and, markov, max, approaches, phi, UNK, indicating, vs,\n",
      "Nearest to d: failure, retirement, pbs, and, wisconsin, aristotle, objectivist, soldier,\n",
      "Nearest to however: american, information, would, vs, attract, people, sheikh, second,\n",
      "Nearest to system: commissioned, motile, wide, himmler, performed, crash, aegean, austin,\n",
      "Nearest to as: box, in, and, by, catch, enveloped, is, matches,\n",
      "Nearest to there: effect, article, reginae, voted, robot, ambients, players, governors,\n",
      "Nearest to its: service, germans, the, vs, women, archie, cl, a,\n",
      "Nearest to only: asterism, areas, shortly, phi, archie, nine, regarding, zero,\n",
      "Nearest to during: garden, vs, pay, nielsen, goal, billion, extends, facilities,\n",
      "Nearest to with: in, and, proposed, ch, extraction, of, quite, on,\n",
      "Loss at iter 11000 : 14.474439855098725\n",
      "Loss at iter 12000 : 13.26585801744461\n",
      "Loss at iter 13000 : 12.588560317516327\n",
      "Loss at iter 14000 : 10.931764296770096\n",
      "Loss at iter 15000 : 10.484398243427277\n",
      "Nearest to he: it, she, who, ballast, and, reaches, molecules, were,\n",
      "Nearest to called: membrane, threats, boasted, wattle, mineral, victoriae, six, asgard,\n",
      "Nearest to for: of, in, and, having, by, dasyprocta, placed, austin,\n",
      "Nearest to if: netherlands, dasyprocta, mauryan, clinton, biscuit, possesses, where, winter,\n",
      "Nearest to state: candidate, facilities, mujahidin, euthanasia, pottery, shem, vs, hbox,\n",
      "Nearest to that: histidine, gate, in, treaty, but, sheridan, trafford, and,\n",
      "Nearest to no: and, a, dasyprocta, approaches, phi, markov, indicating, max,\n",
      "Nearest to d: and, failure, b, retirement, wisconsin, pbs, objectivist, blonde,\n",
      "Nearest to however: agouti, would, information, american, sheikh, that, vs, attract,\n",
      "Nearest to system: commissioned, motile, performed, himmler, abduction, crash, austin, aegean,\n",
      "Nearest to as: in, is, by, and, box, dasyprocta, from, for,\n",
      "Nearest to there: article, effect, khwarizmi, voted, players, percy, qquad, reginae,\n",
      "Nearest to its: the, service, germans, a, women, cl, antoninus, vs,\n",
      "Nearest to only: nn, shortly, asterism, archie, dasyprocta, phi, areas, subkey,\n",
      "Nearest to during: pay, garden, vs, nielsen, goal, agouti, billion, extends,\n",
      "Nearest to with: in, and, of, extraction, ch, by, on, proposed,\n",
      "Loss at iter 16000 : 9.22809316110611\n",
      "Loss at iter 17000 : 8.71582216668129\n",
      "Loss at iter 18000 : 8.807157967329026\n",
      "Loss at iter 19000 : 8.164451275587082\n",
      "Loss at iter 20000 : 7.3571623921394345\n",
      "Nearest to he: it, she, who, they, ballast, were, only, reaches,\n",
      "Nearest to called: membrane, boasted, and, threats, wattle, marian, asgard, mineral,\n",
      "Nearest to for: in, and, of, by, dasyprocta, having, as, to,\n",
      "Nearest to if: netherlands, dasyprocta, clinton, misrepresented, mauryan, where, winter, possesses,\n",
      "Nearest to state: candidate, mujahidin, facilities, shem, vs, pottery, euthanasia, continental,\n",
      "Nearest to that: histidine, but, and, which, gate, sheridan, treaty, in,\n",
      "Nearest to no: a, dasyprocta, approaches, indicating, kick, she, served, and,\n",
      "Nearest to d: b, and, failure, one, retirement, wisconsin, trolling, pbs,\n",
      "Nearest to however: and, would, agouti, information, that, sheikh, attract, chaffee,\n",
      "Nearest to system: commissioned, performed, motile, the, crash, abduction, himmler, jewish,\n",
      "Nearest to as: in, by, and, is, was, box, dasyprocta, for,\n",
      "Nearest to there: he, khwarizmi, article, it, oneself, percy, no, voted,\n",
      "Nearest to its: the, service, their, a, cl, germans, women, vs,\n",
      "Nearest to only: nn, in, shortly, asterism, dasyprocta, archie, regarding, phi,\n",
      "Nearest to during: nielsen, pay, vs, in, garden, extends, integrated, agouti,\n",
      "Nearest to with: in, and, of, extraction, by, for, ch, at,\n",
      "Loss at iter 21000 : 7.26803775882721\n",
      "Loss at iter 22000 : 7.283692680835724\n",
      "Loss at iter 23000 : 7.023638850927353\n",
      "Loss at iter 24000 : 6.951894946813583\n",
      "Loss at iter 25000 : 6.856155541658402\n",
      "Nearest to he: it, she, who, they, chs, there, only, ballast,\n",
      "Nearest to called: membrane, boasted, threats, marian, and, wattle, vincenzo, mineral,\n",
      "Nearest to for: in, of, and, by, with, dasyprocta, from, to,\n",
      "Nearest to if: netherlands, where, dasyprocta, misrepresented, mauryan, clinton, erupted, lilith,\n",
      "Nearest to state: candidate, mujahidin, reuptake, facilities, euthanasia, shem, vs, pottery,\n",
      "Nearest to that: which, histidine, but, sheridan, and, gate, measured, treaty,\n",
      "Nearest to no: a, there, iic, kick, she, conscientious, indicating, and,\n",
      "Nearest to d: b, and, failure, reuptake, tractate, blonde, wisconsin, trolling,\n",
      "Nearest to however: rodin, and, would, that, agouti, sheikh, difranco, information,\n",
      "Nearest to system: commissioned, performed, crash, motile, abduction, jewish, capability, himmler,\n",
      "Nearest to as: by, dasyprocta, and, in, box, is, was, cl,\n",
      "Nearest to there: he, it, and, no, they, oneself, often, percy,\n",
      "Nearest to its: the, their, service, a, cl, propelled, germans, antoninus,\n",
      "Nearest to only: hopwood, nn, shortly, asterism, he, in, archie, regarding,\n",
      "Nearest to during: in, rodin, nielsen, pay, vs, garden, extends, integrated,\n",
      "Nearest to with: in, and, extraction, by, six, for, at, ch,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iter 26000 : 6.483825495958328\n",
      "Loss at iter 27000 : 6.505431483507157\n",
      "Loss at iter 28000 : 5.944219085216522\n",
      "Loss at iter 29000 : 6.254699397087097\n"
     ]
    }
   ],
   "source": [
    "final_embeddings = run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.manifold import TSNE\n",
    "visualize_embeddings(final_embeddings, reverse_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
